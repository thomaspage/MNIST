{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "\n",
    "from skimage import measure\n",
    "from skimage import filters\n",
    "from skimage import exposure\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x = np.loadtxt(\"train_x.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Image segmentation\n",
    "def canny_edges(x):\n",
    "\n",
    "    dim = int(np.sqrt(x.shape[0]))\n",
    "\n",
    "    temp = exposure.adjust_gamma(x, (np.mean(x) ** 2) / 90 / 40)\n",
    "    temp = temp.reshape(-1, dim, dim)\n",
    "    temp = filters.gaussian(temp, 0.5)\n",
    "    temp = (254 * np.divide(temp, np.max(temp))).astype(int)\n",
    "\n",
    "    x = exposure.adjust_gamma(x, 10)\n",
    "    x = x.reshape(-1, dim, dim)\n",
    "    x = filters.gaussian(x, 0.5)\n",
    "    x = (254 * np.divide(x, np.max(x))).astype(int)\n",
    "\n",
    "    contours = measure.find_contours(temp[0], np.mean(temp) + (np.std(temp)), fully_connected='high')\n",
    "\n",
    "    value = 225\n",
    "\n",
    "    while not (len(contours) >= 3):\n",
    "        contours = measure.find_contours(temp[0], value, fully_connected='high')\n",
    "        value -= 1\n",
    "\n",
    "        if value == 50:\n",
    "            break\n",
    "\n",
    "    contours.sort(key=len)\n",
    "    contours = contours[-3:]\n",
    "\n",
    "    point = []\n",
    "    newimages = []\n",
    "\n",
    "    if len(contours) == 3:\n",
    "        for i in range(0, 3):\n",
    "            point.append((np.max(contours[i], axis=0) + np.min(contours[i], axis=0)) / 2)\n",
    "\n",
    "            x_coordinate = int(point[i][0])\n",
    "            if x_coordinate < 14:\n",
    "                x_coordinate = 14\n",
    "            elif x_coordinate > 50:\n",
    "                x_coordinate = 50\n",
    "            else:\n",
    "                x_coordinate = int(point[i][0])\n",
    "\n",
    "            y_coordinate = int(point[i][1])\n",
    "            if y_coordinate < 14:\n",
    "                y_coordinate = 14\n",
    "            elif y_coordinate > 50:\n",
    "                y_coordinate = 50\n",
    "            else:\n",
    "                y_coordinate = int(point[i][1])\n",
    "\n",
    "            newimages.append(x[0][x_coordinate - 14:x_coordinate + 14, y_coordinate - 14:y_coordinate + 14])\n",
    "\n",
    "    else:\n",
    "        newimages = np.zeros((3, 28, 28))\n",
    "\n",
    "    return newimages\n",
    "\n",
    "#Image segmentation\n",
    "newimages = []\n",
    "for i in range(len(x)):\n",
    "    newimages.append(canny_edges(x[i]))\n",
    "    newimages[i] = np.array(newimages[i])\n",
    "    newimages[i] = newimages[i].reshape(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Rewriting x to be combined vectors of the segmented images\n",
    "x=np.empty([len(newimages),len(newimages[0][0])*3])\n",
    "\n",
    "for i in range(0,len(x)):\n",
    "    for k in range(0,3):\n",
    "        for j in range(0,len(newimages[0][0])):\n",
    "            x[i][j+k*len(newimages[0][0])]=newimages[i][k][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load y data\n",
    "y = np.loadtxt(\"train_y.csv\", delimiter=\",\")\n",
    "\n",
    "samp=len(x)\n",
    "y = y.reshape(-1, 1) \n",
    "inputsize=len(x[0])\n",
    "\n",
    "classes=([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 24, 25, 27, 28, 30, 32, 35, 36, 40, 42, 45, 48, 49, 54, 56, 63, 64, 72, 81])\n",
    "classno=len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting x to binary\n",
    "for i in range (0,len(x)):\n",
    "    for j in range (0,len(x[i])):\n",
    "        x[i][j]=round((x[i][j])/255,0)   \n",
    "       \n",
    " #Converting y to one-hot. (0000100000-format)\n",
    "yhot=np.empty([samp,classno])\n",
    "for i in range(0,len(y)):\n",
    "    for j in range(0,classno):\n",
    "        if y[i]==classes[j]:\n",
    "            yhot[i][j]=1\n",
    "        else:\n",
    "            yhot[i][j]=0\n",
    "            \n",
    "yoriginal=y            \n",
    "y=yhot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 100000  Test Accuracy: 0.1475  Training Accuracy: 0.161625\n"
     ]
    }
   ],
   "source": [
    "#To be set by cross-validation:\n",
    "nodes=30\n",
    "hidlays=2\n",
    "traininghorizon=10000000\n",
    "learningrate=1.0\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "#random extract 1/5 of x and y:\n",
    "indices = np.random.permutation(x.shape[0])\n",
    "training_id, test_id = indices[:(samp*4)/5], indices[(samp*4)/5:]\n",
    "trainingx, testx = x[training_id,:], x[test_id,:]\n",
    "trainingy= y[training_id,:]\n",
    "testyoriginal=yoriginal[test_id,:]\n",
    "trainyoriginal=yoriginal[training_id,:]\n",
    "\n",
    "trainsamp=len(trainingx)\n",
    "testsamp=len(testx)\n",
    "\n",
    "#Cross validation loop:\n",
    "for crossval in range(1,2):\n",
    "    #hidlays=crossval\n",
    "    #nodes=crossval*10\n",
    "    #learningrate=0.00001*np.power(10,crossval)\n",
    "    #traininghorizon=np.power(10,crossval)\n",
    "\n",
    "    #Initialize weights to random numbers between -1 and 1\n",
    "    weights=[]\n",
    "    weights.append(2*np.random.random((inputsize,nodes)) - 1)\n",
    "    for i in range(0,hidlays-1):\n",
    "        weights.append(2*np.random.random((nodes,nodes)) - 1)\n",
    "    weights.append(2*np.random.random((nodes,classno)) - 1)\n",
    "\n",
    "    for j in xrange(traininghorizon):\n",
    "        #Reducing training rate half way through:\n",
    "        if j==traininghorizon/2:\n",
    "            learningrate=learningrate/10\n",
    "        \n",
    "        #Pick a training example\n",
    "        ex=np.random.randint(0,trainsamp)\n",
    "        outputlay=hidlays+1\n",
    "\n",
    "        #Feed forward example through network\n",
    "        layers=[]\n",
    "        layers.append(trainingx[ex])\n",
    "        for i in range(0,outputlay):\n",
    "            layers.append(expit(np.dot(layers[i],weights[i])))\n",
    "\n",
    "        errors=[None] * (outputlay+1)\n",
    "\n",
    "        #Compute correction of output\n",
    "        errors[outputlay]=layers[outputlay]*(1-layers[outputlay])*(trainingy[ex]-layers[outputlay])\n",
    "\n",
    "        # Computing share of error for all nodes in each former layer\n",
    "        for i in range(hidlays, 0, -1):\n",
    "            errors[i]=layers[i]*(1-layers[i])*(errors[i+1].dot(weights[i].T))\n",
    "\n",
    "        #Change weights\n",
    "        for i in range(hidlays,-1,-1):\n",
    "            weights[i] +=((layers[i][np.newaxis]).T.dot(errors[i+1][np.newaxis]))*learningrate\n",
    "\n",
    "    testprediction=[]\n",
    "    for testrow in range (0,testsamp):\n",
    "        testestimate=expit(testx[testrow].dot(weights[0]))\n",
    "        for j in range(0,hidlays):\n",
    "            testestimate=expit(testestimate.dot(weights[j+1]))\n",
    "        testprediction.append(np.argmax(testestimate))\n",
    "\n",
    "    testcorrect=0.0\n",
    "    testfalse=0.0\n",
    "    testcorrectpred=[]\n",
    "    testfalsepred=[]\n",
    "\n",
    "    for i in range(0,testsamp):\n",
    "        if testprediction[i]==testyoriginal[i][0]:\n",
    "            testcorrect+=1\n",
    "            testcorrectpred.append(testprediction[i])\n",
    "        else:\n",
    "            testfalse+=1\n",
    "            testfalsepred.append(testprediction[i])\n",
    "\n",
    "    testaccuracy=testcorrect/(testfalse+testcorrect)\n",
    "    \n",
    "    trainprediction=[]\n",
    "    for testrow in range (0,trainsamp):\n",
    "        trainestimate=expit(trainingx[testrow].dot(weights[0]))\n",
    "        for j in range(0,hidlays):\n",
    "            trainestimate=expit(trainestimate.dot(weights[j+1]))\n",
    "        trainprediction.append(np.argmax(trainestimate))\n",
    "\n",
    "    traincorrect=0.0\n",
    "    trainfalse=0.0\n",
    "    traincorrectpred=[]\n",
    "    trainfalsepred=[]\n",
    "\n",
    "    for i in range(0,trainsamp):\n",
    "        if trainprediction[i]==trainyoriginal[i][0]:\n",
    "            traincorrect+=1\n",
    "            traincorrectpred.append(trainprediction[i])\n",
    "        else:\n",
    "            trainfalse+=1\n",
    "            trainfalsepred.append(trainprediction[i])\n",
    "\n",
    "    trainaccuracy=traincorrect/(trainfalse+traincorrect)\n",
    "    \n",
    "    #print(\"Hidden layers: \"+str(hidlays)+\"  Test Accuracy: \"+str(testaccuracy)+\"  Training Accuracy: \"+str(trainaccuracy))\n",
    "    #print(\"Nodes: \"+str(nodes)+\"  Test Accuracy: \"+str(testaccuracy)+\"  Training Accuracy: \"+str(trainaccuracy))\n",
    "    #print(\"Learning rate: \"+str(learningrate)+\"  Test Accuracy: \"+str(testaccuracy)+\"  Training Accuracy: \"+str(trainaccuracy))\n",
    "    print(\"Epochs: \"+str(traininghorizon)+\"  Test Accuracy: \"+str(testaccuracy)+\"  Training Accuracy: \"+str(trainaccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing of test data set\n",
    "xkaggle = np.loadtxt(\"test_x.csv\", delimiter=\",\")\n",
    "\n",
    "newimages = []\n",
    "for i in range(len(xkaggle)):\n",
    "    newimages.append(canny_edges(xkaggle[i]))\n",
    "    newimages[i] = np.array(newimages[i])\n",
    "    newimages[i] = newimages[i].reshape(-1, 28 * 28)\n",
    "    \n",
    "xkaggle=np.empty([len(newimages),len(newimages[0][0])*3])\n",
    "\n",
    "for i in range(0,len(xkaggle)):\n",
    "    for k in range(0,3):\n",
    "        for j in range(0,len(newimages[0][0])):\n",
    "            xkaggle[i][j+k*len(newimages[0][0])]=newimages[i][k][j]\n",
    "\n",
    "kagglesamp=len(xkaggle)\n",
    "y = y.reshape(-1, 1) \n",
    "inputsize=len(xkaggle[0])\n",
    "\n",
    "#Converting x to binary\n",
    "for i in range (0,len(xkaggle)):\n",
    "    for j in range (0,len(xkaggle[i])):\n",
    "        xkaggle[i][j]=round((xkaggle[i][j])/255,0)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the test set:\n",
    "kaggleprediction=[]\n",
    "for testrow in range (0,kagglesamp):\n",
    "    kaggleestimate=expit(xkaggle[testrow].dot(weights[0]))\n",
    "    for j in range(0,hidlays):\n",
    "        kaggleestimate=expit(kaggleestimate.dot(weights[j+1]))\n",
    "    kaggleprediction.append(np.argmax(kaggleestimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Exporting the result\n",
    "with open(\"result.csv\", \"wb\") as file:\n",
    "\twr = csv.writer(file)\n",
    "\tfor i in range(0, len(kaggleprediction)):\n",
    "\t\twr.writerow((i+1,kaggleprediction[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
